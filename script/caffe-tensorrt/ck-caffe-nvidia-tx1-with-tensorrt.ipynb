{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [cknowledge.org/ai](https://cknowledge.org/ai): Crowdsourcing benchmarking and optimisation of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PUBLIC] Benchmarking Caffe and TensorRT on NVIDIA Jetson TX1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** The Caffe results are released with approval from General Motors. The TensorRT 1.0 results are released with approval from NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Overview](#overview)\n",
    "1. [See the code](#code) [for developers]\n",
    "1. [Get the data](#data) [for developers]\n",
    "1. [See the tables](#tables)\n",
    "  1. [All data](#df_all)\n",
    "  1. [All execution time data](#df_time)\n",
    "  1. [Mean execution time per batch](#df_mean_time_per_batch)\n",
    "  1. [Mean execution time per image](#df_mean_time_per_image)\n",
    "  1. [Best mean execution time per image](#df_best_mean_time_per_image)\n",
    "1. [See the graphs - grouped by models](#plot_models)\n",
    "  1. [All libs](#plot_models_all)\n",
    "  1. [GPU libs](#plot_models_gpu)\n",
    "  1. [CUDA-level performance libs](#plot_models_cuda)\n",
    "  1. [cuBLAS libs](#plot_models_cublas)\n",
    "  1. [cuDNN libs](#plot_models_cudnn)\n",
    "  1. [TensorRT engine](#plot_models_tensorrt)\n",
    "1. [See the graphs - grouped by libs](#plot_libs)\n",
    "  1. [All models](#plot_libs_all)\n",
    "  1. [All models, GPU libs](#plot_libs_gpu)\n",
    "  1. [Models with AlexNet-level accuracy](#plot_libs_alexnet)\n",
    "  1. [Models with AlexNet-level accuracy, CPU lib](#plot_libs_alexnet_cpu) \n",
    "  1. [Models with AlexNet-level accuracy, CUDA-level perfomance libs](#plot_libs_alexnet_cuda)\n",
    "  1. [Models with AlexNet-level accuracy, fp16 libs](#plot_libs_alexnet_fp16)\n",
    "  1. [GoogleNet, fp16 libs](#plot_libs_googlenet_fp16)\n",
    "1. [See the graphs - per layer execution time profiling](#plot_per_layer)\n",
    "1. [See the graphs - the ideal adaptive solution](#plot_ideal)\n",
    "  1. [Using all libs for adaptation](#plot_ideal_all)\n",
    "  1. [Using CUDA-level performance libs for adaptation](#plot_ideal_cuda)\n",
    "  1. [Using cuDNN and cuBLAS for adaptation](#plot_ideal_cudnn_cublas)\n",
    "  1. [Using cuDNN and libDNN for adaptation](#plot_ideal_cudnn_libdnn)\n",
    "1. [See the memory consumption graph](#plot_memory)\n",
    "  1. [Balance memory consumption and execution time per image](#balance_memory_time)\n",
    "1. [Compare AlexNet and SqueezeNet 1.1](#alexnet_vs_squeezenet)\n",
    "  1. [Compare memory consumption](#alexnet_vs_squeezenet_memory)\n",
    "  1. [Compare execution time](#alexnet_vs_squeezenet_time)\n",
    "1. [Conclusion](#conclusion)\n",
    "  1. [What are the improvements brought on by each approach?](#improvements_of_each_approach)\n",
    "  1. [How do dividiti's results compare with NVIDIA's results in the whitepaper?](#compare_with_whitepaper)\n",
    "  1. [How does TX1 compare to Myriad2?](#tx1_vs_myriad2)\n",
    "1. [Crowdsourcing benchmarking and optimisation of AI](#cknowledge_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook compares the performance (execution time, memory consumption):\n",
    "- on **[dividiti](http://dividiti.com)**'s Jetson TX1 board ([official page](http://www.nvidia.com/object/jetson-tx1-dev-kit.html), [Phoronix review](http://www.phoronix.com/scan.php?page=article&item=nvidia-jtx1-perf)):\n",
    "  - CPU:\n",
    "    - ARM&reg; Cortex&reg;-A57 architecture (\"big\");\n",
    "    - 4 cores;\n",
    "    - Max clock 1743 MHz;\n",
    "  - GPU:\n",
    "    - Maxwell&trade; architecture;\n",
    "    - 256 CUDA cores;\n",
    "    - Max clock 998 MHz;\n",
    "  - RAM:\n",
    "    - LPDDR4;\n",
    "    - 4 GB (shared between the CPU and the GPU);\n",
    "    - Max bandwidth 25.6 GB/s;\n",
    "  - [Linux for Tegra 24.2.1](http://developer.download.nvidia.com/embedded/L4T/r24_Release_v2.1/Docs/Tegra_Linux_Driver_Package_Release_Notes_R24.2.1.pdf);\n",
    "  - [JetPack 2.3.1](http://docs.nvidia.com/jetpack-l4t/index.html#developertools/desktop/jetpack/l4t/2.3.1);\n",
    "  - CUDA Toolkit 8.0.33.\n",
    "```\n",
    "$ uname -a\n",
    "Linux tegra-ubuntu 3.10.96-tegra #1 SMP PREEMPT Wed Nov 9 19:42:57 PST 2016 aarch64 aarch64 aarch64 GNU/Linux\n",
    "$ cat /etc/lsb-release\n",
    "DISTRIB_ID=Ubuntu\n",
    "DISTRIB_RELEASE=16.04\n",
    "DISTRIB_CODENAME=xenial\n",
    "DISTRIB_DESCRIPTION=\"Ubuntu 16.04.1 LTS\"\n",
    "```\n",
    "\n",
    "- using 6 Caffe **libraries**:\n",
    "  - [`tag`] **Branch** (**revision hash, date**): **math libraries**.\n",
    "  - [`cpu`] Master ([24d2f67](https://github.com/BVLC/caffe/commit/24d2f67173db3344141dce24b1008efffbfe1c7d), 28/Nov/2016): with [OpenBLAS 0.2.19](https://github.com/xianyi/OpenBLAS/releases/tag/v0.2.19);\n",
    "  - [`nvidia-cuda`] NVIDIA 0.15 ([1024d34](https://github.com/NVIDIA/caffe/commit/1024d34d93cd34a9013d6fac4e56e45162073d38), 17/Nov/2016): with [cuBLAS](https://developer.nvidia.com/cublas) (part of CUDA Toolkit 8.0.33);\n",
    "  - [`nvidia-cudnn`] NVIDIA 0.15 ([1024d34](https://github.com/NVIDIA/caffe/commit/1024d34d93cd34a9013d6fac4e56e45162073d38), 17/Nov/2016): with [cuDNN 5.1](https://developer.nvidia.com/cudnn);\n",
    "  - [`nvidia-fp16-cuda`] NVIDIA experimental/fp16 ([fca1cf4](https://github.com/NVIDIA/caffe/commit/fca1cf475d1d0a6d355f8b9877abcc4e13951c9c), 11/Jul/2016): with [cuBLAS](https://developer.nvidia.com/cublas) (part of CUDA Toolkit 8.0.33);\n",
    "  - [`nvidia-fp16-cudnn`] NVIDIA experimental/fp16 ([fca1cf4](https://github.com/NVIDIA/caffe/commit/fca1cf475d1d0a6d355f8b9877abcc4e13951c9c), 11/Jul/2016): with [cuDNN 5.1](https://developer.nvidia.com/cudnn);\n",
    "  - [`libdnn-cuda`] OpenCL ([b735c2d](https://github.com/BVLC/caffe/commit/b735c2dd4103ac963332b400168507dd7cefd204), 23/Nov/2016): with [libDNN](https://github.com/BVLC/caffe/issues/4155) and [cuBLAS](https://developer.nvidia.com/cublas) (**NB:** not yet tuned for TX1 - uses optimal parameters for GTX 1080);\n",
    "\n",
    "- using 2 configurations of the [NVIDIA TensorRT 1.0.0 EA engine](https://developer.nvidia.com/tensorrt):\n",
    "  - [`tensorrt-fp16`] NVIDIA TensorRT 1.0.0 EA with fp16 enabled;\n",
    "  - [`tensorrt-fp32`] NVIDIA TensorRT 1.0.0 EA with fp16 disabled;\n",
    "  <br>\n",
    "  **NB:** This EA (\"early access\") version is used in accordance with special licensing terms: the results are released with explicit written approval from NVIDIA. The results may not be representative of the GA (\"general availability\") version.\n",
    "\n",
    "- using 4 CNN **models**:\n",
    "  - [GoogleNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet);\n",
    "  - [AlexNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet);\n",
    "  - [SqueezeNet 1.0](https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.0);\n",
    "  - [SqueezeNet 1.1](https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1);\n",
    "\n",
    "- with the **batch size** varying from 2 to 16 with step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the execution time metric to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw = [ 'forward' ]\n",
    "fwbw = [ 'forward', 'backward' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set to fw for inference; to fwbw for training.\n",
    "direction = fw\n",
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if direction==fw:\n",
    "    time_ms = 'time_fw_ms'\n",
    "else: # direction==fwbw\n",
    "    time_ms = 'time_fwbw_ms'\n",
    "time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def images_per_second(time_in_milliseconds):\n",
    "    return 1000.0 / time_in_milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## Data wrangling code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scientific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some of the scientific packages are missing, please install them using:\n",
    "```\n",
    "# pip install jupyter pandas numpy matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython as ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('IPython version: %s' % ip.__version__)\n",
    "print ('Pandas version: %s' % pd.__version__)\n",
    "print ('NumPy version: %s' % np.__version__)\n",
    "print ('Matplotlib version: %s' % mp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collective Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CK is not installed, please install it using:\n",
    "```\n",
    "# pip install ck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ck.kernel as ck\n",
    "print ('CK version: %s' % ck.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repo_uoa = 'ck-caffe-nvidia-tx1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_experimental_results(repo_uoa, tags):\n",
    "    module_uoa = 'experiment'\n",
    "    r = ck.access({'action':'search', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'tags':tags})\n",
    "    if r['return']>0:\n",
    "        print (\"Error: %s\" % r['error'])\n",
    "        exit(1)\n",
    "    experiments = r['lst']\n",
    "    \n",
    "    dfs = []\n",
    "    for experiment in experiments:\n",
    "        data_uoa = experiment['data_uoa']\n",
    "        r = ck.access({'action':'list_points', 'repo_uoa':repo_uoa, 'module_uoa':module_uoa, 'data_uoa':data_uoa})\n",
    "        if r['return']>0:\n",
    "            print (\"Error: %s\" % r['error'])\n",
    "            exit(1)\n",
    "\n",
    "        # Get (lib_tag, model_tag) from a list of tags that should be available in r['dict']['tags'].\n",
    "        # Tags include 2 of the 3 irrelevant tags, a model tag and a lib tag.\n",
    "        # NB: Since it's easier to list all model tags than all lib tags, the latter list is not expicitly specified.\n",
    "        tags = r['dict']['tags']\n",
    "        irrelevant_tags = [ 'explore-batch-size-libs-models','time_gpu','time_cpu','time_gpu_fp16' ]\n",
    "        model_tags = [ 'bvlc-alexnet','bvlc-googlenet','deepscale-squeezenet-1.0','deepscale-squeezenet-1.1' ]\n",
    "        lib_model_tags = [ tag for tag in tags if tag not in irrelevant_tags ]\n",
    "        model_tags = [ tag for tag in lib_model_tags if tag in model_tags ]\n",
    "        lib_tags = [ tag for tag in lib_model_tags if tag not in model_tags ]\n",
    "        if len(lib_tags)==1 and len(model_tags)==1:\n",
    "            (lib, model) = (lib_tags[0], model_tags[0])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for point in r['points']:\n",
    "            with open(os.path.join(r['path'], 'ckp-%s.0001.json' % point)) as point_file:\n",
    "                point_data_raw = json.load(point_file)\n",
    "                # Obtain column data.\n",
    "                characteristics = [\n",
    "                    {\n",
    "                        'time (ms)'   : characteristics['run'].get(time_ms,+1e9), # \"positive infinity\"\n",
    "                        'memory (MB)' : characteristics['run'].get('memory_mbytes',-1),\n",
    "                        'success?'    : characteristics['run'].get('run_success','n/a'),\n",
    "                        'per layer info' : characteristics['run'].get('per_layer_info',[])\n",
    "                    }\n",
    "                    for characteristics in point_data_raw['characteristics_list'] \n",
    "                ]\n",
    "                # Deal with missing column data (resulting from failed runs).\n",
    "                if len(characteristics)==1:\n",
    "                    repetitions = point_data_raw['features'].get('statistical_repetitions',1)\n",
    "                    characteristics = characteristics * repetitions\n",
    "                # Construct a DataFrame.\n",
    "                df = pd.DataFrame(characteristics)\n",
    "                # Set columns and index names.\n",
    "                df.columns.name = 'run characteristic'\n",
    "                df.index.name = 'repetition'\n",
    "                # Set indices.\n",
    "                if lib=='tensorrt-1.0.0':\n",
    "                    enable_fp16 = (point_data_raw['choices']['env']['CK_TENSORRT_ENABLE_FP16'] != 0)\n",
    "                    df['lib'] = 'tensorrt-fp%d' % (16 if enable_fp16 else 32)\n",
    "                else:\n",
    "                    df['lib'] = lib\n",
    "                df['model'] = model\n",
    "                df['batch size'] = point_data_raw['choices']['env']['CK_CAFFE_BATCH_SIZE']\n",
    "                df = df.set_index(['lib', 'model', 'batch size'], append=True)\n",
    "                df = df.reorder_levels(('model', 'lib', 'batch size', 'repetition'))\n",
    "                # Append to the list of similarly constructed DataFrames.\n",
    "                dfs.append(df)\n",
    "    # Concatenate all constructed DataFrames (i.e. stack on top of each other).\n",
    "    result = pd.concat(dfs)\n",
    "    return result.sortlevel(result.index.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image or memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(mean, std, title='Execution time per image (ms)', ymax=0, rot=0):\n",
    "    ymax = mean.max().max() if ymax==0 else ymax\n",
    "    mean.plot(yerr=std, ylim=[0,ymax*1.05], title=title,\n",
    "        kind='bar', rot=rot, figsize=[16, 8], grid=True, legend=True, colormap=cm.autumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot maximum number of images per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretty_print_libs = {\n",
    "    'cpu': 'OpenBLAS (CPU)',\n",
    "    'libdnn-cuda':'libDNN-fp32',\n",
    "    'nvidia-cuda':'cuBLAS-fp32',\n",
    "    'nvidia-fp16-cuda':'cuBLAS-fp16',\n",
    "    'nvidia-cudnn':'cuDNN-fp32',\n",
    "    'nvidia-fp16-cudnn':'cuDNN-fp16',\n",
    "    'tensorrt-fp32':'TensorRT-fp32',\n",
    "    'tensorrt-fp16':'TensorRT-fp16'\n",
    "}\n",
    "\n",
    "pretty_print_models = {\n",
    "    'bvlc-alexnet':'AlexNet',\n",
    "    'bvlc-googlenet':'GoogleNet',\n",
    "    'deepscale-squeezenet-1.0':'SqueezeNet 1.0',\n",
    "    'deepscale-squeezenet-1.1':'SqueezeNet 1.1'\n",
    "}\n",
    "\n",
    "speedup_sort_models = [\n",
    "    'OpenBLAS (CPU)',\n",
    "    'libDNN-fp32',\n",
    "    'cuBLAS-fp32',\n",
    "    'cuBLAS-fp16',\n",
    "    'cuDNN-fp32',\n",
    "    'cuDNN-fp16',\n",
    "    'TensorRT-fp32',\n",
    "    'TensorRT-fp16'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ['cuda', 'cudnn'] are roughly equivalent to ['nvidia-cuda', 'nvidia-cudnn'], so can be dropped.\n",
    "def plot_max_num_images_per_second(df_mean_time_per_image, libs_to_drop=['cuda', 'cudnn'], rot=0):\n",
    "    min_time_per_image = df_mean_time_per_image.min(axis=1).unstack('lib')\n",
    "    max_num_images_per_second = images_per_second(min_time_per_image) \\\n",
    "        .drop(libs_to_drop, axis=1) \\\n",
    "        .rename(columns=pretty_print_libs, index=pretty_print_models) \\\n",
    "        .reindex(columns=speedup_sort_models)\n",
    "    ax = max_num_images_per_second \\\n",
    "            .plot(title='Images/s (with the best even batch size between 2 and 16)', kind='bar',\n",
    "                  figsize=[16, 8], width=0.95, rot=rot, grid=True, legend=True, colormap=cm.autumn)\n",
    "    for patch in ax.patches:\n",
    "         ax.annotate(str(int(patch.get_height()+0.5)), (patch.get_x()*1.00, patch.get_height()*1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the speedup over a given baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['cuda', 'cudnn'] are roughly equivalent to ['nvidia-cuda', 'nvidia-cudnn'], so can be dropped.\n",
    "def plot_speedup_over_baseline(df_mean_time_per_image, baseline='cpu', libs_to_drop=['cuda', 'cudnn'], rot=0):\n",
    "    speedup_over_baseline = df_mean_time_per_image.min(axis=1).unstack('model').ix[baseline] / \\\n",
    "                            df_mean_time_per_image.min(axis=1).unstack('model')\n",
    "    speedup_over_baseline = speedup_over_baseline.T \\\n",
    "        .drop(libs_to_drop, axis=1) \\\n",
    "        .rename(index=pretty_print_models, columns=pretty_print_libs) \\\n",
    "        .reindex(columns=speedup_sort_models)\n",
    "    ax = speedup_over_baseline \\\n",
    "            .plot(title='Speedup over the given baseline (%s)' % pretty_print_libs[baseline], kind='bar',\n",
    "                  figsize=[16, 8], width=0.95, rot=rot, grid=True, legend=True, colormap=cm.autumn)\n",
    "    for patch in ax.patches:\n",
    "        ax.annotate('{0:.2f}'.format(patch.get_height())[0:4], (patch.get_x()*1.00, patch.get_height()*1.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This transformation is time consuming, hence only call it once for multiple plots.\n",
    "def get_per_layer_info(df_all):\n",
    "    df_per_layer_info = df_all['per layer info']\n",
    "    row_dfs = []\n",
    "    for (row_info, row_id) in zip(df_per_layer_info, range(len(df_per_layer_info))):\n",
    "        # Skip constructing a DataFrame when no layer info is available.\n",
    "        if not row_info: continue\n",
    "        # Augment each layer info with the row index: (model, lib, batch size, repetition).\n",
    "        for layer_info in row_info:\n",
    "            layer_info.update({ k : v for k, v in zip(df_per_layer_info.index.names, df_per_layer_info.index[row_id]) })\n",
    "        # Construct a DataFrame and move the row index to where it belongs.\n",
    "        row_df = pd.DataFrame(data=row_info).set_index(df_per_layer_info.index.names)\n",
    "        row_dfs.append(row_df)\n",
    "    return pd.concat(row_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_time_per_image_per_layer(df_per_layer_info, model, libs, batch_sizes,\n",
    "                                  direction=['forward'], lower=0.0, upper=1.0, ymax=0, rot=90):\n",
    "    df_time_per_batch = df_per_layer_info.loc[model, libs, batch_sizes] \\\n",
    "        .set_index(['direction', 'label'], append=True) \\\n",
    "        .reorder_levels(['direction', 'label', 'model', 'lib', 'batch size', 'repetition' ]) \\\n",
    "        .ix[direction] \\\n",
    "        .reorder_levels(['label', 'model', 'lib', 'batch size', 'repetition', 'direction' ]) \\\n",
    "        .groupby(level=['label', 'model', 'lib', 'batch size', 'repetition']).sum() \\\n",
    "        ['time_ms']\n",
    "    df_time_per_image = df_time_per_batch.unstack('batch size') / batch_sizes\n",
    "    df = df_time_per_image.unstack(['lib', 'model'])\n",
    "    df = df.reorder_levels(['model', 'lib', 'batch size'], axis=1)\n",
    "    mean = df.groupby(level='label').mean()\n",
    "    std = df.groupby(level='label').std()\n",
    "    select = (lower*mean.sum() <= mean).any(axis=1) & (mean <= upper*mean.sum()).any(axis=1)\n",
    "    ymax = mean[select].max().max() if ymax==0 else ymax\n",
    "    plot(mean=mean[select], std=std[select], title='Execution per image time per layer (ms)', ymax=ymax, rot=rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ideal adaptive solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The ideal adaptive solution for each layer selects the best performing library from the 'libs_for_adaptation' list.\n",
    "# FIXME: add batch_sizes as explicit parameter.\n",
    "def get_ideal_adaptive_solution(df_per_layer_info, libs_for_adaptation, direction):\n",
    "    df_for_adaptation = df_per_layer_info \\\n",
    "        .set_index(['direction', 'label'], append=True) \\\n",
    "        .reorder_levels(['direction', 'lib', 'model', 'label', 'batch size', 'repetition']) \\\n",
    "        .ix[direction] \\\n",
    "        .reorder_levels(['lib', 'model', 'label', 'batch size', 'repetition', 'direction']) \\\n",
    "        .ix[libs_for_adaptation] \\\n",
    "        .reorder_levels(['model', 'label', 'lib', 'batch size', 'repetition', 'direction']) \\\n",
    "        ['time_ms']\n",
    "    # With every step, reduce the rightmost dimension until the min time per model is reached.\n",
    "    df_cum_time_per_repetition = df_for_adaptation.groupby(level=df_for_adaptation.index.names[:-1]).sum()\n",
    "    df_min_time_per_repetition = df_cum_time_per_repetition.groupby(level=df_cum_time_per_repetition.index.names[:-1]).min()\n",
    "    df_min_time_per_batch = df_min_time_per_repetition.unstack('batch size') / batch_sizes\n",
    "    df_min_time_per_image = df_min_time_per_batch.min(axis=1)\n",
    "    df_min_time_per_layer = df_min_time_per_image.groupby(level=df_min_time_per_image.index.names[:-1]).min()\n",
    "    #df_min_time_per_model = df_min_time_per_layer.groupby(level=df_min_time_per_layer.index.names[:-1]).sum()\n",
    "    # Transform to get the models in the index and the libs in the columns.\n",
    "    df_min_time_per_layer_idx = df_min_time_per_image.groupby(level=df_min_time_per_image.index.names[:-1]).idxmin()\n",
    "    df_ideal = df_min_time_per_image[df_min_time_per_layer_idx] \\\n",
    "        .reorder_levels(['model', 'lib', 'label']) \\\n",
    "        .groupby(level=['model', 'lib']).sum() \\\n",
    "        .unstack('lib')\n",
    "    # Sort in the order of increasing time per model.\n",
    "    df_ideal_sorted = df_ideal.ix[df_ideal.sum(axis=1).sort_values(ascending=True).index]\n",
    "    return df_ideal_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ideal_adaptive_solution(df_ideal, df_real, tag=\"\"):\n",
    "    figsize=[15, 3]\n",
    "    if not tag==\"\": figsize=[10, 2] # good for dumping png (e.g. 3 graphs fit well onto a slide).\n",
    "    for model in df_ideal.index:\n",
    "        df_data = {}; df_data['adaptive'] = df_ideal.ix[model]\n",
    "        for lib in df_ideal.columns:\n",
    "            df_data[lib] = pd.Series(index=df_ideal.columns)\n",
    "            df_data[lib][lib] = df_real.ix[model, lib]\n",
    "        df = pd.DataFrame(df_data).T \\\n",
    "            .rename(index={'cpu': 'OpenBLAS only', 'cuda':'cuBLAS only', 'cudnn':'cuDNN only', 'libdnn-cuda': 'libDNN only'},\n",
    "                    columns={'cpu': 'OpenBLAS', 'cuda':'cuBLAS', 'cudnn':'cuDNN', 'libdnn-cuda': 'libDNN'})\n",
    "        ax = df.ix[df.sum(axis=1).sort_values(ascending=True).index] \\\n",
    "            .plot(title='%s - execution time per image (ms)' % model, kind='barh', stacked=True,\n",
    "                  grid=True, legend=True, colormap=cm.summer_r, figsize=figsize, width=0.9) \\\n",
    "            .legend(loc='lower right')\n",
    "        if not tag==\"\": ax.get_figure().savefig('%s.%s.png' % (tag, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot execution time per image and memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_time_per_image_and_memory_consumption(df_all, model, lib):\n",
    "    df = df_all[['time (ms)', 'memory (MB)']] \\\n",
    "        .groupby(level=df_all.index.names[:-1]).mean() \\\n",
    "        .loc[model, lib]\n",
    "    df['time per image (ms)'] = df['time (ms)'].divide(df.index, axis=0)\n",
    "    df['memory per image (MB)'] = df['memory (MB)'].divide(df.index, axis=0)\n",
    "    df = df.drop('time (ms)', axis=1).sortlevel(axis=1)\n",
    "    ax = df.plot(secondary_y=['memory (MB)', 'memory per image (MB)'], title='%s w/ %s' % (model, lib),\n",
    "                 figsize=[12, 8], mark_right=False, colormap=cm.winter, grid=True)\n",
    "    ax.set_ylabel('execution time (ms)'); ax.legend(loc='center left'); ax.set_ylim(0)\n",
    "    ax.right_ax.set_ylabel('memory consumption (MB)'); ax.right_ax.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Get the experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Please ignore this section if you are not interested in re-running or modifying this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Caffe experimental data was collected on the experimental platform (after installing all Caffe libraries and models of interest) as follows:\n",
    "```\n",
    "$ cd `ck find ck-caffe:script:explore-batch-size-libs-models`\n",
    "$ python explore-batch-size-libs-models-benchmark.py\n",
    "```\n",
    "It can be downloaded from GitHub via CK as follows:\n",
    "```\n",
    "$ ck pull repo:ck-caffe-nvidia-tx1 --url=https://github.com/dividiti/ck-caffe-nvidia-tx1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tables\"></a>\n",
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_all\"></a>\n",
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = get_experimental_results(repo_uoa=repo_uoa, tags='explore-batch-size-libs-models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_all.columns)\n",
    "pd.options.display.max_rows = len(df_all.index)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_time\"></a>\n",
    "### All execution time data indexed by repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_time = df_all['time (ms)'].unstack(df_all.index.names[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_time.columns)\n",
    "pd.options.display.max_rows = len(df_time.index)\n",
    "df_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_mean_time_per_batch\"></a>\n",
    "### Mean execution time per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_batch = df_time.describe().ix['mean'].unstack(level='batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_mean_time_per_batch.columns)\n",
    "pd.options.display.max_rows = len(df_mean_time_per_batch.index)\n",
    "df_mean_time_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_sizes = df_mean_time_per_batch.columns.tolist()\n",
    "# batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_mean_time_per_image\"></a>\n",
    "### Mean execution time per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_image = df_mean_time_per_batch / batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = len(df_mean_time_per_image.columns)\n",
    "pd.options.display.max_rows = len(df_mean_time_per_image.index)\n",
    "df_mean_time_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"df_best_mean_time_per_image\"></a>\n",
    "### Best mean execution time per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean_time_per_image.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_max_num_images_per_second(df_mean_time_per_image, libs_to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the batch size that gives the minimum time per image (or the maximum number of images per second)?\n",
    "df_mean_time_per_image.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Focus on e.g. nvidia-fp16-cuda, for which the batch size of 16 is not always the best.\n",
    "df_mean_time_per_image.idxmin(axis=1).reorder_levels(['lib', 'model']).loc['nvidia-fp16-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Is the same answer as via .min(axis=1).values?\n",
    "# df_mean_time_per_image.lookup(df_mean_time_per_image.index, df_mean_time_per_image.idxmin(axis=1)) \\\n",
    "#     == df_mean_time_per_image.min(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_time_per_image = df_time / (batch_sizes*(len(df_time.columns)/len(batch_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_min_time_per_image_index = pd.DataFrame(df_mean_time_per_image.idxmin(axis=1)).set_index(0, append=True).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_lib = df_time_per_image[df_min_time_per_image_index] \\\n",
    "    .stack(['model', 'lib']).reorder_levels(['model','lib','repetition']).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_lib_mean = df_model_lib.groupby(level=['model', 'lib']).mean()\n",
    "df_model_lib_std  = df_model_lib.groupby(level=['model', 'lib']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_positive_infinity = df_model_lib_mean > 1e5\n",
    "df_model_lib_mean[zero_positive_infinity] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude_positive_infinity = df_model_lib_mean < 1e6\n",
    "# df_model_lib_mean = df_model_lib_mean[exclude_positive_infinity]\n",
    "# df_model_lib_std = df_model_lib_std[exclude_positive_infinity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models\"></a>\n",
    "## Plot by Caffe models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_all\"></a>\n",
    "### All libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('lib')\n",
    "std  = df_model_lib_std.unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_gpu\"></a>\n",
    "### Only GPU libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('lib').drop('cpu', axis=1)\n",
    "std  = df_model_lib_std.unstack('lib').drop('cpu', axis=1)\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cuda\"></a>\n",
    "### Only GPU libs with CUDA-level fp32 performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda_level_performance = ['nvidia-cuda', 'nvidia-cudnn', 'libdnn-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cublas\"></a>\n",
    "### Only GPU libs using cuBLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cublas_libs = ['nvidia-cuda', 'nvidia-fp16-cuda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cublas_libs].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cublas_libs].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuBLAS, NVIDIA's fp16 branch is up to 20% faster than NVIDIA's fp32 mainline.\n",
    "nvidia_fp16_cuda_vs_nvidia_fp32_cuda = mean['nvidia-fp16-cuda'] / mean['nvidia-cuda']\n",
    "nvidia_fp16_cuda_vs_nvidia_fp32_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_cudnn\"></a>\n",
    "### Only GPU libs using cuDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cudnn_libs = ['nvidia-cudnn', 'nvidia-fp16-cudnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[cudnn_libs].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[cudnn_libs].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With cuDNN, NVIDIA's fp16 branch is up to 35% (roughly one third) faster than NVIDIA's fp32 mainline.\n",
    "nvidia_fp16_cudnn_vs_nvidia_fp32_cudnn = mean['nvidia-fp16-cudnn'] / mean['nvidia-cudnn']\n",
    "nvidia_fp16_cudnn_vs_nvidia_fp32_cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_models_tensorrt\"></a>\n",
    "### TensorRT vs cuDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libs = [ 'nvidia-cudnn', 'nvidia-fp16-cudnn', 'tensorrt-fp32', 'tensorrt-fp16' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.reorder_levels(['lib', 'model'])[libs].unstack('lib')\n",
    "std = df_model_lib_std.reorder_levels(['lib', 'model'])[libs].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT using fp16 is roughly twice as fast as TensorRT using fp32.\n",
    "tensorrt_fp16_vs_tensorrt_fp32 = mean['tensorrt-fp16'] / mean['tensorrt-fp32']\n",
    "tensorrt_fp16_vs_tensorrt_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT using fp32 is up to 54% faster than cuDNN using fp32.\n",
    "tensorrt_fp32_vs_cudnn_fp32 = mean['tensorrt-fp32'] / mean['nvidia-cudnn']\n",
    "tensorrt_fp32_vs_cudnn_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT using fp16 is up to 63% faster than cuDNN using fp16.\n",
    "tensorrt_fp16_vs_cudnn_fp16 = mean['tensorrt-fp16'] / mean['nvidia-fp16-cudnn']\n",
    "tensorrt_fp16_vs_cudnn_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT using fp16 up to 4 times faster than cuDNN using fp32.\n",
    "tensorrt_fp16_vs_cudnn_fp32 = mean['tensorrt-fp16'] / mean['nvidia-cudnn']\n",
    "tensorrt_fp16_vs_cudnn_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs\"></a>\n",
    "## Plot by Caffe libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_all\"></a>\n",
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('model')\n",
    "std  = df_model_lib_std.unstack('model')\n",
    "plot(mean, std, rot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_gpu\"></a>\n",
    "### All models, only GPU libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = df_model_lib_mean.unstack('model').drop('cpu', axis=0)\n",
    "std  = df_model_lib_std.unstack('model').drop('cpu', axis=0)\n",
    "plot(mean, std, rot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet\"></a>\n",
    "### Only models with AlexNet-level accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alexnet_level_accuracy = ['bvlc-alexnet','deepscale-squeezenet-1.0','deepscale-squeezenet-1.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On this platform with all the libraries, SqueezeNet 1.0 is always slower than AlexNet\n",
    "# despite a 50x reduction in weights (5 MB vs. 250 MB).\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model')\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model')\n",
    "plot(mean, std, rot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_cpu\"></a>\n",
    "### Only models with AlexNet-level accuracy, only CPU lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.1 is 46% faster than AlexNet with OpenBLAS (on the CPU).\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[['cpu']]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[['cpu']]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_cuda\"></a>\n",
    "### Only models with AlexNet-level accuracy, only libs with CUDA-level performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.0 is slower than AlexNet. SqueezeNet 1.1 is 28% faster than AlexNet with \n",
    "# libDNN-CUDA, and roughly equivalent to AlexNet with cuBLAS and cuDNN.\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[cuda_level_performance]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[cuda_level_performance]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_alexnet_fp16\"></a>\n",
    "### Only models with AlexNet-level accuracy, only fp16 libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet 1.1 achieves > 500 inferences per second with TensorRT using fp16.\n",
    "mean = df_model_lib_mean[alexnet_level_accuracy].unstack('model').ix[['nvidia-fp16-cudnn', 'tensorrt-fp16']]\n",
    "std  = df_model_lib_std[alexnet_level_accuracy].unstack('model').ix[['nvidia-fp16-cudnn', 'tensorrt-fp16']]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean['deepscale-squeezenet-1.1'] / mean['bvlc-alexnet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_libs_googlenet_fp16\"></a>\n",
    "### Only GoogleNet, only fp16 libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GoogleNet achieves nearly 200 inferences per second with TensorRT using fp16.\n",
    "mean = df_model_lib_mean[['bvlc-googlenet']].unstack('model').ix[['nvidia-fp16-cudnn', 'tensorrt-fp16']]\n",
    "std  = df_model_lib_std[['bvlc-googlenet']].unstack('model').ix[['nvidia-fp16-cudnn', 'tensorrt-fp16']]\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT using fp16 is more than twice as fast than cuDNN using fp16.\n",
    "mean.ix['tensorrt-fp16'] / mean.ix['nvidia-fp16-cudnn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_per_second(mean['bvlc-googlenet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our results (with the batch size of 16) are very close to NVIDIA's results (with the batch size of 64).\n",
    "Image(url=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/09/Figure_2-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_per_layer\"></a>\n",
    "## Plot execution time per image per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_per_layer_info = get_per_layer_info(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.options.display.max_columns = len(df_per_layer_info.columns)\n",
    "# pd.options.display.max_rows = len(df_per_layer_info.index)\n",
    "# df_per_layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of batch sizes.\n",
    "# NB: This suggests that the fully connected layers benefit the most from larger batch sizes.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs='nvidia-cuda',\n",
    "                              batch_sizes=[2, 8, 16], direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of batch sizes. Only plot layers that consume at least 10% of the total execution time.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs='nvidia-cudnn',\n",
    "                              batch_sizes=[8, 16], direction=direction, lower=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs.\n",
    "# NB: cuDNN and cuBLAS perform about the same on the fully connected layers (which suggests that\n",
    "# cuDNN falls back to cuBLAS for these).\n",
    "# Unsurprisingly, cuDNN performs better than cuBLAS on the convolution layers.\n",
    "# Surprisingly, cuBLAS performs a bit better than cuDNN on the relu layers.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs=['nvidia-cuda','nvidia-cudnn'],\n",
    "                              batch_sizes=16, direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs.\n",
    "# NB: This suggests that libDNN is faster than cuDNN on the conv1 and expand1x1 layers, but slower on the squeeze1x1, \n",
    "# expand3x3, conv/pool10 layers. (Recall that libDNN is not yet tuned for TX1 but uses parameters optimal for GTX 1080.)\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='deepscale-squeezenet-1.1', libs=['nvidia-cudnn', 'libdnn-cuda'],\n",
    "                              batch_sizes=16, direction=direction, ymax=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs. Only plot layers that consume between 5% and 10% of the total execution time.\n",
    "# NB: libDNN is slower than cuDNN on the expand3x3 layers and conv10 layers, but a bit faster on the conv1 layer.\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='deepscale-squeezenet-1.1', libs=['nvidia-cudnn', 'libdnn-cuda'],\n",
    "                              batch_sizes=16, direction=direction, lower=0.05, upper=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot for a list of libs and a list of batch sizes. (This works but might not be terribly legible).\n",
    "plot_time_per_image_per_layer(df_per_layer_info, model='bvlc-alexnet', libs=['nvidia-cudnn', 'nvidia-cuda'],\n",
    "                              batch_sizes=[4,6], direction=direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal\"></a>\n",
    "## Plot the ideal adaptive solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, using cuDNN typically results in the minimum execution time. For some layers, however, other libraries may outperform cuDNN (e.g. libDNN from the OpenCL branch of Caffe). As we show below, using the best performing library per layer results in up to 17% execution time reduction over using cuDNN alone. For other models and on other platforms such adaptation can potentially results even in higher savings (e.g. up to 22% on the GTX 1080)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Currently, the savings are hypothetical. However, Caffe allows for manual adaptation, i.e. the user can specify the engine to use for each layer in the model file (`*.prototxt`). We are working on generating the optimized model file automatically from the obtained ideal adaptive solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_all\"></a>\n",
    "### Using all reasonable libs for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_libs = df_per_layer_info.index.get_level_values('lib').drop_duplicates() \\\n",
    "    .drop(['nvidia-fp16-cuda', 'nvidia-fp16-cudnn', 'tensorrt-fp16', 'tensorrt-fp32'])\n",
    "all_libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row specifies an ideal adaptive solution for a model. Each column specifies the execution time (in ms per image) that the ideal adaptive solution would cumulatively spend using a particular library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_all = get_ideal_adaptive_solution(df_per_layer_info, all_libs, direction)\n",
    "df_ideal_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_all, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 17% execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_all.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cuda\"></a>\n",
    "### Using CUDA-level performance libs for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cuda = get_ideal_adaptive_solution(df_per_layer_info, cuda_level_performance, direction)\n",
    "df_ideal_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cuda, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cuda.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 0.1% worse performance when using the CUDA-level performance libs only.\n",
    "df_ideal_cuda.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cudnn_cublas\"></a>\n",
    "### Using cuDNN and cuBLAS for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cudnn_cublas = get_ideal_adaptive_solution(df_per_layer_info, ['nvidia-cudnn', 'nvidia-cuda'], direction)\n",
    "df_ideal_cudnn_cublas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cudnn_cublas, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cudnn_cublas.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Up to 8% worse performance when using cuDNN+cuBLAS only.\n",
    "df_ideal_cudnn_cublas.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_ideal_cudnn_libdnn\"></a>\n",
    "### Using cuDNN and libDNN for adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ideal_cudnn_libdnn = get_ideal_adaptive_solution(df_per_layer_info, ['nvidia-cudnn', 'libdnn-cuda'], direction)\n",
    "df_ideal_cudnn_libdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ideal_adaptive_solution(df_ideal_cudnn_libdnn, df_model_lib_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hypothetical execution time reduction compared to the best non-adaptive solution (i.e. cuDNN).\n",
    "df_best_lib = df_model_lib_mean.reorder_levels(['lib', 'model'])[cuda_level_performance].unstack('lib')\n",
    "df_ideal_cudnn_libdnn.sum(axis=1) / df_best_lib.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Less than 2% worse performance when using cuDNN+libDNN only.\n",
    "df_ideal_cudnn_libdnn.sum(axis=1) / df_ideal_all.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plot_memory\"></a>\n",
    "## Plot memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_memory = df_all['memory (MB)']\n",
    "# Batch size of 4; repetition 0 (should always be available).\n",
    "df_memory = df_memory.unstack(['model','lib']).loc[4].loc[0].unstack('lib')\n",
    "plot(mean=df_memory, std=pd.DataFrame(), title='Memory size (MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"balance_memory_time\"></a>\n",
    "### Balance memory consumption and execution time per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above, however, does not tell the full story. The memory consumption, as reported by Caffe, increases linearly with the batch size. In other words, the memory consumption per image is constant. (Note that extra memory may be required e.g. for GPU buffers in host memory.)\n",
    "\n",
    "The execution time per image, however, decreases asymptotically. Since minimizing the execution time almost always should be balanced with minimizing the memory consumption, we should select the batch size that results in \"good enough\" performance.\n",
    "\n",
    "We give several examples below. Note that the execution time per batch is omitted to make the execution time per image more pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 8 \"good enough\"?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'bvlc-alexnet', 'nvidia-cudnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 2 \"good enough\"?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'deepscale-squeezenet-1.1', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 10 \"good enough\"?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'bvlc-googlenet', 'tensorrt-fp32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is the batch size of 8 \"good enough\" (below 2 ms per image)?\n",
    "plot_time_per_image_and_memory_consumption(df_all, 'deepscale-squeezenet-1.1', 'tensorrt-fp16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet\"></a>\n",
    "## Compare AlexNet and SqueezeNet 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet_memory\"></a>\n",
    "### Memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SqueezeNet consumes about 4 times more memory than AlexNet.\n",
    "df_memory.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].iloc[1] / \\\n",
    "df_memory.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"alexnet_vs_squeezenet_time\"></a>\n",
    "### Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using TensorRT, SqueezeNet is over 1.5 times faster than AlexNet.\n",
    "mean = df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib')\n",
    "std  = df_model_lib_std[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib')\n",
    "plot(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using TensorRT, SqueezeNet is over 1.5 times faster than AlexNet.\n",
    "df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib').iloc[1] / \\\n",
    "df_model_lib_mean[['bvlc-alexnet', 'deepscale-squeezenet-1.1']].unstack('lib').iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"improvements_of_each_approach\"></a>\n",
    "### What are the improvements brought on by each approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT-fp16 is up to 69x faster than the CPU.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image, baseline='cpu', libs_to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cuDNN-fp16 is about 2x faster than cuBLAS-fp32. TensorRT-fp16 is up to 5.2x faster than cuBLAS-fp32.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image, baseline='nvidia-cuda', libs_to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TensorRT-fp16 is up to 2.7x faster than cuDNN-fp16.\n",
    "plot_speedup_over_baseline(df_mean_time_per_image, baseline='nvidia-fp16-cudnn', libs_to_drop=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AlexNet and SqueezeNet 1.1 have very similar performance with cuBLAS and cuDNN. (They also have very similar accuracy!)\n",
    "# SqueezeNet, however, benefits much more from TensorRT optimizations, becoming over 1.5 times faster than AlexNet.\n",
    "# At the same time, SqueezeNet requires about 4 times more memory than AlexNet (at least, with Caffe), so it's a trade-off.\n",
    "plot_speedup_over_baseline(\n",
    "    df_mean_time_per_image.ix[['bvlc-alexnet', 'deepscale-squeezenet-1.1']], baseline='nvidia-cuda', libs_to_drop=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compare_with_whitepaper\"></a>\n",
    "### How do dividiti's results compare with NVIDIA's results in the whitepaper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Nov/2015, NVIDIA published a [whitepaper](https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf) entitled \"GPU-Based Deep Learning Inference: A Performance and Power Analysis\", which presented the throughput of inference (images per second) on AlexNet and GoogleNet using small and large batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several points of difference complicate direct comparison:\n",
    "- TX1 board support package: NVIDIA's is before Nov/2015; dividiti's is Nov/2016. [In a [pinned post](https://devtalk.nvidia.com/default/topic/935300/jetson-tx1/deep-learning-inference-performance-validation-on-tx1/) on NVIDIA's Jetson forum published in May/2016, the following are mentioned: Linux for Tegra 23.1 (vs 24.2.1); CUDA Toolkit 7.0.73 (vs 8.0.33); cuDNN 4 (vs 5.1).]\n",
    "- Caffe source: NVIDIA's is probably a mix of open-source and proprietary, before Nov/2015; dividiti's is public, Nov/2016;\n",
    "- \"small batch size\": NVIDIA's is 1; dividiti's is 2;\n",
    "- \"large batch size\": NVIDIA's is 128 (AlexNet) and 64 (GoogleNet); dividiti's is 16;\n",
    "- GPU clock frequency: NVIDIA's is 690 MHz (presumably to demonstrate better power consumption); dividiti's is 998 MHz (to demonstrate maximum performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_data = []\n",
    "nvidia_data.append({\n",
    "    'source':'nvidia', 'model':'bvlc-alexnet', 'batch size':1, 'clock':'690 MHz',\n",
    "    'fp32 (images/s)':47, 'fp16 (images/s)':67\n",
    "})\n",
    "nvidia_data.append({\n",
    "    'source':'nvidia', 'model':'bvlc-alexnet', 'batch size':128, 'clock':'690 MHz',\n",
    "    'fp32 (images/s)':155, 'fp16 (images/s)':258 \n",
    "})\n",
    "nvidia_data.append({\n",
    "    'source':'nvidia', 'model':'bvlc-googlenet', 'batch size':1, 'clock':'690 MHz',\n",
    "    'fp32 (images/s)':33, 'fp16 (images/s)':33\n",
    "})\n",
    "nvidia_data.append({       \n",
    "    'source':'nvidia', 'model':'bvlc-googlenet', 'batch size':64, 'clock':'690 MHz',\n",
    "    'fp32 (images/s)':52, 'fp16 (images/s)':75\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dividiti_data = []\n",
    "dividiti_data.append({\n",
    "    'source':'dividiti', 'model':'bvlc-alexnet', 'batch size':2, 'clock':'998 MHz',\n",
    "    'fp32 (images/s)':(1000/(df_time['bvlc-alexnet','nvidia-cudnn',2].mean()/2)),\n",
    "    'fp16 (images/s)':(1000/(df_time['bvlc-alexnet','nvidia-fp16-cudnn',2].mean()/2)),\n",
    "})\n",
    "dividiti_data.append({\n",
    "    'source':'dividiti', 'model':'bvlc-alexnet', 'batch size':16, 'clock':'998 MHz',\n",
    "    'fp32 (images/s)':(1000/(df_time['bvlc-alexnet','nvidia-cudnn',16].mean()/16)),\n",
    "    'fp16 (images/s)':(1000/(df_time['bvlc-alexnet','nvidia-fp16-cudnn',16].mean()/16)),\n",
    "})\n",
    "dividiti_data.append({\n",
    "    'source':'dividiti', 'model':'bvlc-googlenet', 'batch size':2, 'clock':'998 MHz',\n",
    "    'fp32 (images/s)':(1000/(df_time['bvlc-googlenet','nvidia-cudnn',2].mean()/2)),\n",
    "    'fp16 (images/s)':(1000/(df_time['bvlc-googlenet','nvidia-fp16-cudnn',2].mean()/2)),\n",
    "})\n",
    "dividiti_data.append({\n",
    "    'source':'dividiti', 'model':'bvlc-googlenet', 'batch size':16, 'clock':'998 MHz',\n",
    "    'fp32 (images/s)':(1000/(df_time['bvlc-googlenet','nvidia-cudnn',16].mean()/16)),\n",
    "    'fp16 (images/s)':(1000/(df_time['bvlc-googlenet','nvidia-fp16-cudnn',16].mean()/16)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(nvidia_data+dividiti_data).set_index(['model','batch size','source','clock']).sortlevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale dividiti's data.\n",
    "pd.concat([\n",
    "    pd.DataFrame(nvidia_data).set_index(['model','batch size','source']).drop(labels='clock',axis=1),\n",
    "    pd.DataFrame(dividiti_data).set_index(['model','batch size','source']).drop(labels='clock',axis=1)*(690.0/998.0)\n",
    "]).sortlevel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scaling dividiti's data to the lower frequency of 690 MHz, NVIDIA's fp16 performance figures look in order. NVIDIA's fp32 performance figure on GoogleNet using the batch size of 1 (33 images/s) is 50% higher than expected (a copy-and-paste error from the fp16 figure?). NVIDIA's performance figures on AlexNet using the batch size of 128 also seem to be on the high side given the observed trend of improvements tailing off pretty rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tx1_vs_myriad2\"></a>\n",
    "### How does TX1 compare to Myriad2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[According to Movidius](http://uploads.movidius.com/1461814467-Fathom-Combined-2-pager.pdf), using fp16 their Myriad2 processor runs GoogleNet at 15 images per second (~67 ms per image), while consumes about 1 Watt of power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[According to the NVIDIA 2015 whitepaper](https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf), using fp16 their TX1 processor runs GoogleNet at up to 75 images per second, while consumes up to 6 Watts of power. This would make TX1 and Myriad2 roughly comparable in terms of images per second per unit power: ~13 vs 15 images per second per Watt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the performance improvements brought by TensorRT (<a href=\"#plot_libs_googlenet_fp16\">\n",
    "~2x vs cuDNN</a>) swing the comparison in favour of NVIDIA even for small batch sizes: ~18-22 images per second per Watt, according to their [blog introducing TensorRT](https://devblogs.nvidia.com/parallelforall/jetpack-doubles-jetson-tx1-deep-learning-inference/blog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/09/Figure_3-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cknowledge_ai\"></a>\n",
    "## [cknowledge.org/ai](https://cknowledge.org/ai): Crowdsourcing benchmarking and optimisation of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of open-source tools for [collecting knowledge on optimising AI](http://bit.ly/hipeac49-ckdl):\n",
    "\n",
    "\n",
    "* [Android app](https://play.google.com/store/apps/details?id=openscience.crowdsource.video.experiments&hl=en_GB)\n",
    "* [Desktop app](https://github.com/dividiti/ck-crowdsource-dnn-optimization)\n",
    "* [CK-Caffe](https://github.com/dividiti/ck-caffe)\n",
    "* [CK-TensorRT](https://github.com/dividiti/ck-tensorrt)\n",
    "* [CK-TensorFlow](https://github.com/ctuning/ck-tensorflow)\n",
    "* etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
